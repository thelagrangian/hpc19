\documentclass[amsmath,amssymb]{revtex4}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[amsmath,thmmarks]{ntheorem}
\usepackage{graphicx}% Include figure files
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{longtable}
%\usepackage{slashbox}
\usepackage[colorlinks]{hyperref}
%\setcounter{section}{-1}
\usepackage{pgfplots}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{plotmarks}

\usepackage{listings}
\usepackage{xcolor} % for setting colors


% set the default code style
\lstset{
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=4, % tab space width
    showstringspaces=true, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red}, % string color
    %basicstyle=\footnotesize,
    basicstyle=\ttfamily\footnotesize,
}



\begin{document}

\title{Spring 2019: Advanced Topics in Numerical Analysis High Performance Computing\\
Implementation of Fast Fourier Transform (FFT)}
\author{Yongyan Rao, yr780@nyu.edu}
%\email{yr780@nyu.edu}
\date{\today}
\maketitle


\section{\label{sec:sec0}0. Introduction}

The Fourier transform of a function $f: \mathbb{R}\to\mathbb{C}$ is defined as
\begin{equation}
\hat{f}(k)=\int^{\infty}_{-\infty}f(x)e^{-2\pi ixk}dx.
\end{equation}
And the inverse Fourier transform is defined as
\begin{equation}
f(x)=\int^{\infty}_{-\infty}\hat{f}(k)e^{2\pi ixk}dk.
\end{equation}
The discrete Fourier transform (DFT) of a sequence of $N$ complex numbers $\{x_0, x_1, \cdots, x_{N-1}\}$ is defined as
\begin{equation}
y_k = \sum^{N-1}_{n=0}x_ne^{-\frac{2\pi i}{N}kn}, k = 0, 1, \cdots, N-1.
\end{equation}
From the definition, we can find that a straightforward implementation of DFT is of complexity of $\mathcal{O}(N^2)$. By observing and preserving the underlying symmetry of DFT, a fast Fourier transform (FFT) algorithm is able to reduce the complexity to $\mathcal{O}(N\log N)$, without losing any accuracy.

The most commonly used FFT algorithm is the Cooley-Tukey algorithm, which uses the divide and conquer paradigm that recursively factorizes $N$ and applies the transform on a lower scale. To simplify the discussion, we only consider the radix-2 FFT, which means $N$ is a power of 2, i.e., $N=2^l, l\in\mathbb{N}$. The radix-2 FFT can be derived as follows.
\begin{equation*}
\begin{aligned} 
y_k\equiv y_{k,0,1} &= \sum^{N-1}_{n=0}x_ne^{-\frac{2\pi i}{N}kn} = \sum^{N-1}_{{\rm even}~n=0}x_ne^{-\frac{2\pi i}{N}kn} + \sum^{N-1}_{{\rm odd}~n=0}x_ne^{-\frac{2\pi i}{N}kn}\\
&= \sum^{\frac{N}{2}-1}_{n=0}x_{2n}e^{-\frac{2\pi i}{N}k(2n)} + \sum^{\frac{N}{2}-1}_{n=0}x_{2n+1}e^{-\frac{2\pi i}{N}k(2n+1)}\\
&= \sum^{\frac{N}{2}-1}_{n=0}x_{2n}e^{-\frac{2\pi i}{N/2}kn} + e^{-\frac{2\pi i}{N}k}\sum^{\frac{N}{2}-1}_{n=0}x_{2n+1}e^{-\frac{2\pi i}{N/2}kn}\\
&=y_{k,0,2} + e^{-\frac{2\pi i}{N}k}y_{k,1,2},
\end{aligned} 
\end{equation*}
where $y_{k,i,j}$ represents the $k$-th term of the transform of $x_n$, using summation beginning from $i$-th term with stride $j$. The recurrence relation of $y$ can be written as
\begin{equation*}
\begin{aligned} 
y_{k,0,1} &= y_{k,0,2} + e^{-\frac{2\pi i}{N}k}y_{k,1,2}\\
                                &=y_{k,0,4} + e^{-\frac{2\pi i}{N}k}(y_{k,1,4}+y_{k,2,4})+e^{-2\frac{2\pi i}{N}k}y_{k,3,4}=\cdots.\\
\end{aligned} 
\end{equation*}

Therefore, it is straightforward to implement the FFT algorithm in terms of recursion. However, since it is easier to parallelize iteration than recursion in OpenMP's paradigm, we use the iterative Cooley-Tukey algorithm in the discussion as follows.
\begin{lstlisting}[language=C, caption={Iterative Cooley-Tukey algorithm for FFT}, label=list:list1]
algorithm fft
  //input: an array x of complex numbers of length $n$, with $n$ is a power of 2.
  //output: an array y of complex numbers of length $n$, which is the FFT of x.
  
  y = bit-reverse(x);
  
  for(m = 2; m <= n; m *= 2)
    theta = 2 * pi/m;
    omega_m = exp(i * theta); //Euler's formula exp(i * theta) = cos(theta) + i * sin(theta)
    for(k = 0; k < n; k += m)
      omega = 1;
      for(j = 0; j < m/2; ++j)
        t = omega * y[k + j + m/2];
        u = y[k + j];
        y[k + j] = u + t;
        y[k + j + m/2] = u - t;
        omega = omega * omega_m;

  return y;
\end{lstlisting}

\subsection{\label{subsec:subsec1.1}Note on the Bit Reverse function}
In Listing \ref{list:list1}, we still need to further discuss the implementation details about the bit reverse function on line 5. The bit reverse is defined as reversing the bits in the binary representation of an unsigned integer. Therefore, after the bit reverse, the most significant bit becomes the least significant bit, and vice versa, the least significant bit becomes the most significant bit. For example, the bit reserve of an 8-bit unsigned integer $00000100_2(=4_{10})$ is $00100000_2(=32_{10})$. It is clear that for a $n$-bit unsigned integer $x$, calculating its bit reverse is of complex of $\log n$. If the number $n$ is fixed in the problem, the bit reverse function can be implemented as a lookup table. Otherwise, it needs to be implemented explicitly in the program. It is good to observe that the bit reversing of the sequence $0, 1, \cdots, 2^n-1$ is a permutation of the sequence, which implies that the bit reverse function can be fully parallelized. The coded bit reverse function is modified from \cite{ref:bitreverse}.

The rest of the paper is organized as follows. Section \ref{sec:sec1} discusses the sequential implementation of the FFT. Section \ref{sec:sec2} discusses the multithreading solution implemented using OpenMP. Section \ref{sec:sec3} presents a naive cuda implementation of the FFT. And Section  \ref{sec:sec4} concludes the paper by summarizing the findings of the study.

\section{\label{sec:sec1}1. Sequential implementation of FFT}
We first implemented the sequential version of FFT, with $\mathcal{O}(n\log n)$ complexity.
There are several points worth noticing as follows.
\begin{enumerate}
\item A fast implementation of sine and cosine functions is used, which covers the domain of $[-\pi, \pi]$. It is sufficient in term of the use of FFT.
\item When the problem size is relatively large, there exists numerical discrepancy between the results of the implementation and the GNU Scientific Library (GSL). For example, when the problem size is 67,108,864, there are 48 such instances that the relative error of either the real or the imaginary part is greater than $10^{-3}$, and it always happens at the points where the absolute value of the result is small. It is very rare when the problem size is less than this scale. Furthermore, using sine and cosine functions from {\tt C} language's {\tt math.c} library does not remedy the discrepancy significantly.
\end{enumerate}

\section{\label{sec:sec2}2. OpenMP implementation of FFT}
Although the algorithm above has been adapted to the form of iteration, it still keeps the trace of recursion, which means not all the iterations are independent. It is clear that the bit-reverse-copy function, line 5, has independent iterations, and the loop on line 10 has independent iterations. We can use OpenMP to parallel these portions of FFT. The program was compiled and run on cuda2.cims, which has two (2) 20-core CPUs. Since the OpenMP code does not explicitly indicate the CPU usage, the program would run on one of the CPUs, which means it could utilize 20 cores. The followings are the time measures, FIG. \ref{fig:fig1}, and the speedup of the OpenMP implementation, FIG. \ref{fig:fig2}.

The speedup measured is never greater than 10, given the program is run on a 20-core machine. The phenomenon is understandable in terms of Amdahl's law. The outermost and innermost loops, line 7 and line 12, respectively, in the algorithm are iteration dependent, which cannot be parallelized. They serialize a great portion of the program.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.45\textwidth}

\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Problem size,
  ylabel=Time (s),
  xmode=log,
  ymode=log,
  ]
  
\addplot [mark=triangle, color=green, mark size=2pt]
table {./plot/seq.data};
\addlegendentry[font=\tiny]{sequential}

\addplot [mark=square, color=red, mark size=2pt]
table {./plot/gsl.data};
\addlegendentry[font=\tiny]{gsl}

\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/omp.data};
\addlegendentry[font=\tiny]{omp}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig1}Running time of FFT implementations}
\end{minipage}
~
\begin{minipage}[b]{0.45\textwidth}
\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Problem size,
  ylabel=Speedup,
  xmode=log,
  ymode=log,
  ]
  
\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/speedup.data};
\addlegendentry[font=\tiny]{speedup: sequential/omp}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig2}Speedup of OpenMP FFT}
\end{minipage}
\end{figure}

\newpage
One could also study the scalability of the OpenMP implementation. A program is weakly scalable, if when the number of processes/threads increases $x$ times, the program completes an $x$ times larger problem within the same amount of time. Therefore, from FIG. \ref{fig:fig3}, we can conclude that the OpenMP implementation of FFT is not weakly scalable.

A program is strongly scalable, if when the number of processes/threads increases $x$ times, the program completes a same-size problem $x$ faster, which means the speedup of the program is proportional to the number of processes/threads. Therefore, from FIG. \ref{fig:fig4}, we can conclude that the OpenMP implementation of FFT is not strongly scalable.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.45\textwidth}

\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Number of threads,
  ylabel=Time (s),
  xmode=log,
  ymode=log,
  ]


\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/weak.data};
\addlegendentry[font=\tiny]{Running time}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig3}OpenMP FFT weak scaling study}
\end{minipage}
~
\begin{minipage}[b]{0.45\textwidth}
\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Number of threads,
  ylabel=Speedup,
  xmode=log,
  ymode=log,
  ]
  
\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/strong.data};
\addlegendentry[font=\tiny]{Actual speedup}

\addplot [mark=o, color=green, mark size=2pt]
table {./plot/strongref.data};
\addlegendentry[font=\tiny]{Ideal strong scalable speedup}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig4}OpenMP FFT strong scaling study}
\end{minipage}
\end{figure}


\section{\label{sec:sec3}3. cuda implementation of FFT}
A straightforward implementation of cuda FFT is coded, and tested on cuda2.cims. There are several points worth mentioning as follows.
\begin{enumerate}
\item In the implementation, memory copy happens twice. One is copying input from host to device before the first kernel launch, and the other is copying output from device to host after the last kernel run.
\item Due to the distribution of the parallelizable regions, the bit reverse function in line 5 and the loop in line 10, two (2) kernels are implemented, and two (2) kernel launches happen in the program execution. The first kernel is used as the bit reverse function, which only needs to be launched once, i.e., the first kernel launch. The second kernel is used for the parallel computing of the loop of line 10.
\item Because the line 10 loop is nested in the outermost loop of line 7, the second kernel needs to be launched multiple times, $\log n$, where $n$ is the problem size. 
\item The program was complied and run on cuda2.cims, and we noticed that to run properly, the greatest problem size the program accepted was 268,435,456. There exist four (4) {\tt double} arrays in device memory during a kernel run, with two (2) input arrays and two (2) output arrays. Two components for either the input or the output represent the real part and the imaginary part of a complex number, respectively. Therefore, for a problem of size $n$, it needs $8\times4\times n$ byte device memory, which means a problem size 268,435,456 corresponds to $8.6\times10^9$ byte device memory. The device memory was measured during the program run, which showed that the device's total memory was $11.5\times10^9$ byte and free memory was $9.2\times10^9$ byte. Therefore, the phenomenon was due to device memory limit.
\item The performance of the cuda implementation was measured and presented in FIG. \ref{fig:fig5} and FIG. \ref{fig:fig6}. We could notice that from the measurement that the cuda implementation always took longer time than the original sequential implementation. The best relative performance of the cuda implementation was only about $\frac{3}{4}$ of the performance of the sequential implementation.
\item It is not a very great surprise for the cuda implementation to have such poor performance. As discussed above, the iterative Cooley-Tukey algorithm is not fully parallelizable. This fact has the following implications.
\begin{enumerate}
\item The second kernel needs to be launched multiple times, $\log n$. Furthermore, in the first iterations, when $m$ is small, the kernel has less operations to execute in each launch, which makes it more memory bounded.
\item The memory access pattern for the loop in line 10 makes it difficult to fully exploit the coalesced memory access. This is because when $m$ is large, the threads next to each other are actually trying to access memory locations that are with $m$ locations apart from each other, which means combining multiple memory accesses into a single transaction is impossible in the situation.
\item The naive cuda implementation of FFT does not exploit the use of shared memory. And instead, data is directly read from and written to device main memory. To make use of the lower latency shared memory, the original Cooley-Tukey algorithm needs to be modified to suit the GPU architecture.
\end{enumerate}
\end{enumerate}

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.45\textwidth}

\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Problem size,
  ylabel=Time (s),
  xmode=log,
  ymode=log,
  ]


\addplot [mark=square, color=red, mark size=2pt]
table {./plot/cudaseq.data};
\addlegendentry[font=\tiny]{sequential}

\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/cuda.data};
\addlegendentry[font=\tiny]{cuda}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig5}Running time of FFT implementations}
\end{minipage}
~
\begin{minipage}[b]{0.45\textwidth}
\begin{tikzpicture}
\begin{axis}[
  legend pos=north west,
  legend cell align=left,
  xlabel=Problem size,
  ylabel=Speedup,
  xmode=log,
  ymode=log,
  ]
  
\addplot [mark=o, color=blue, mark size=2pt]
table {./plot/cudaspeedup.data};
\addlegendentry[font=\tiny]{speedup: sequential/cuda}

\end{axis}
\end{tikzpicture}
\caption{\label{fig:fig6}Speedup of cuda FFT}
\end{minipage}
\end{figure}

\section{\label{sec:sec4}4. Conclusion}



\medskip

\begin{thebibliography}{9}

\bibitem{ref:bitreverse} 
Bit reverse, \url{http://www.katjaas.nl/bitreversal/bitreversal.html}.

\bibitem{ref:bookscwmaa} 
Jakub Kurzak, David A. Bader, Jack Dongarra, \textit{Scientific Computing with Multicore and Accelerators}, Chapman \& Hall/CRC, 2010.

\end{thebibliography}



\end{document}
